{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning implementations which complete localization OR classifications tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-abdfba21b3ac>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MIO-TCD-Classification/train/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MIO-TCD-Classification/train/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MIO-TCD-Classification/train/t10k-images-idx3-ubyte.gz\n",
      "Extracting MIO-TCD-Classification/train/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Training set (images) shape: (55000, 784)\n",
      "Training set (labels) shape: (55000, 10)\n",
      "Test set (images) shape: (10000, 784)\n",
      "Test set (labels) shape: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'(Label: pedestrian)')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# motivation of code below was found on https://www.datacamp.com/community/tutorials/cnn-tensorflow-python\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu\n",
    "\n",
    "data = input_data.read_data_sets('MIO-TCD-Classification/train',one_hot=True)\n",
    "\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\n",
    "print(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))\n",
    "\n",
    "#dictionary with class names and corresponding class labels\n",
    "label_dict = {\n",
    " 0: 'articulated_truck',\n",
    " 1: 'background',\n",
    " 2: 'bicycle',\n",
    " 3: 'bus',\n",
    " 4: 'car',\n",
    " 5: 'motorcycle',\n",
    " 6: 'non-motorized_vehicle',\n",
    " 7: 'pedestrian',\n",
    " 8: 'pickup_truck',\n",
    " 9: 'single_unit_truck',\n",
    "}\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(data.train.images[8], (28,28))\n",
    "curr_lbl = np.argmax(data.train.labels[0,:])\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(data.test.images[0], (28,28))\n",
    "curr_lbl = np.argmax(data.test.labels[0,:])\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.images[0]\n",
    "np.max(data.train.images[0])\n",
    "np.min(data.train.images[0])\n",
    "\n",
    "# Reshape training and testing image\n",
    "train_X = data.train.images.reshape(-1, 28, 28, 1)\n",
    "test_X = data.test.images.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 10), (10000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = data.train.labels\n",
    "test_y = data.test.labels\n",
    "train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iters = 30 \n",
    "learning_rate = 0.001 \n",
    "batch_size = 128\n",
    "\n",
    "# define network parameters \n",
    "\n",
    "# here wedefine number of inputs \n",
    "# MNIST data input (img shape: 28*28)\n",
    "n_input = 28\n",
    "\n",
    "# MNIST total classes (0-9 digits)\n",
    "n_classes = 10\n",
    "\n",
    "#both placeholders are of type float\n",
    "x = tf.placeholder(\"float\", [None, 28,28,1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x) \n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.get_variable('M0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wc2': tf.get_variable('M1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wc3': tf.get_variable('M2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wd1': tf.get_variable('M3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'out': tf.get_variable('M6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('P0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc2': tf.get_variable('P1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc3': tf.get_variable('P2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bd1': tf.get_variable('P3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable('P4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases):  \n",
    "\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Output, class prediction\n",
    "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-989f812044df>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = conv_net(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "#calculate accuracy across all the given images and average them out. \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 0.009128, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.97500\n",
      "Iter 1, Loss= 0.005268, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98080\n",
      "Iter 2, Loss= 0.002693, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98610\n",
      "Iter 3, Loss= 0.001607, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98840\n",
      "Iter 4, Loss= 0.001672, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98690\n",
      "Iter 5, Loss= 0.001582, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99020\n",
      "Iter 6, Loss= 0.000546, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98900\n",
      "Iter 7, Loss= 0.000748, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98980\n",
      "Iter 8, Loss= 0.003814, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98700\n",
      "Iter 9, Loss= 0.000588, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98630\n",
      "Iter 10, Loss= 0.000711, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98910\n",
      "Iter 11, Loss= 0.000021, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99010\n",
      "Iter 12, Loss= 0.000011, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98760\n",
      "Iter 13, Loss= 0.000863, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98880\n",
      "Iter 14, Loss= 0.000428, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98860\n",
      "Iter 15, Loss= 0.000017, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98790\n",
      "Iter 16, Loss= 0.000018, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98940\n",
      "Iter 17, Loss= 0.000246, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99070\n",
      "Iter 18, Loss= 0.000020, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99030\n",
      "Iter 19, Loss= 0.018961, Training Accuracy= 0.99219\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99270\n",
      "Iter 20, Loss= 0.000049, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99140\n",
      "Iter 21, Loss= 0.000008, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99090\n",
      "Iter 22, Loss= 0.000012, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99030\n",
      "Iter 23, Loss= 0.000004, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99080\n",
      "Iter 24, Loss= 0.000362, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99080\n",
      "Iter 25, Loss= 0.000003, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99150\n",
      "Iter 26, Loss= 0.000054, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99080\n",
      "Iter 27, Loss= 0.000001, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99300\n",
      "Iter 28, Loss= 0.000401, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99190\n",
      "Iter 29, Loss= 0.000028, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99170\n",
      "Iter 30, Loss= 0.000055, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98980\n",
      "Iter 31, Loss= 0.000013, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99150\n",
      "Iter 32, Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99230\n",
      "Iter 33, Loss= 0.000010, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99030\n",
      "Iter 34, Loss= 0.000002, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99180\n",
      "Iter 35, Loss= 0.000001, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99320\n",
      "Iter 36, Loss= 0.000017, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99220\n",
      "Iter 37, Loss= 0.000026, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98730\n",
      "Iter 38, Loss= 0.000001, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99270\n",
      "Iter 39, Loss= 0.000003, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99140\n",
      "Iter 40, Loss= 0.000004, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99280\n",
      "Iter 41, Loss= 0.000009, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99270\n",
      "Iter 42, Loss= 0.000335, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99120\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "    for i in range(training_iters):\n",
    "        for batch in range(len(train_X)//batch_size):\n",
    "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
    "            # Run optimization op (backprop).\n",
    "                # Calculate batch loss and accuracy\n",
    "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
    "                                                              y: batch_y})\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y})\n",
    "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for all 10000 mnist test images\n",
    "        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
    "        train_loss.append(loss)\n",
    "        test_loss.append(valid_loss)\n",
    "        train_accuracy.append(acc)\n",
    "        test_accuracy.append(test_acc)\n",
    "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
    "plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test loss')\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')\n",
    "plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Schematic of architecture (1 point) \n",
    "\n",
    "Uses Convolution Neural Network using the TensorFlow framework for classification.  \n",
    "first we load the data using one hot encoding,\n",
    "Secondly, we analyze the data \n",
    "Next, we construct the deep neaural network model using 3 convolutional layers: \n",
    "     1. 32-3 x 3 filters \n",
    "     2. 64-3 x 3 filters \n",
    "     3. 128-3 x 3 filters \n",
    "     4. Additionally, there are 3 max pooling layers of size 2x2 each \n",
    "then we train and test the model, \n",
    "then we train our model with dropouts inserted(to avoid overfitting), \n",
    "Finally, we make predicitions of the test data. \n",
    "\n",
    "2. Description of training(2 points) \n",
    "We first define our training iterations (50), a learning rate(0.001) with a fixed batch size(128). \n",
    "Next we use placeholders: x is an input placeholder and y holds the label of the training images in a form of a matrix\n",
    "We then define convolution and max pooling functions with certain weights and bias parameters\n",
    "after the data passes these layers, we flatten the output and connect the flattened neurons with each neuron in the next layer.\n",
    "In the last layer, we have 11 neurons to classify 11 labels, \n",
    "Finally we apply the adam optimizer\n",
    "To train: we launch the graph and run the session to execute the variables and run the tensor. Then we use two for loops, one of training iterations and one for batch size. Finally we feed the placeholders x and y the actual data; they then return the loss and accuracy. \n",
    "\n",
    "3. Evaluation of performance(1 point) \n",
    "\n",
    "Here we have to put the testing accuracy percentage \n",
    "\n",
    "4. Description of validation(3 points) \n",
    "\n",
    "5. Camparision with the methods from sections 2&3(1 point) \n",
    "\n",
    "5. Code with a description of the environment (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
